{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKv6-X-oi1OK"
      },
      "source": [
        "# Reddit\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVx6VsSLmF8r"
      },
      "source": [
        "## Prerequistes\n",
        "\n",
        "To scrape Reddit, we will be using the Praw library. This library is a wrapper around Reddit's API, so you will first need to create a Reddit account and create an application. From that, you will need to copy the client-id and client-secret for your application. A tutorial on how to do so can be found [here](https://www.youtube.com/watch?v=0mGpBxuYmpU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKbu1IZGlp-5"
      },
      "source": [
        "First, install the Praw library in your environment using ```pip install praw```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TW0HAtszdld",
        "outputId": "63e03490-6cad-4e99-8a08-cddd58495d72"
      },
      "outputs": [],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkGFngFWjQME"
      },
      "source": [
        "Now we will want to actually import the Praw library into our code. We will also ```import io``` at this time so that we can eventually load the information we collect into a ```.csv``` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPfRP2L5zfHA"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDetOhvamXZp"
      },
      "source": [
        "## Collecting Information\n",
        "\n",
        "Now we'll get to the interesting stuff. First we will create an instance of the Praw wrapper. Here is where you will need to paste the ```client_id``` and ```client_secret``` you copied earlier. You will also need to include the ```username``` and ```password``` of the Reddit account that contains the application secrets.\n",
        "\n",
        "\n",
        "With this instance, we can grab a subreddit through the ```reddit.subreddit``` function and then iterate through different submissions/posts in the subreddit. In this example, we iterate through hottest submissions in r/worldnews, but we could have also gotten the newest submissions using ```subreddit.new()```, or the top submissions using ```subreddit.top()```. A full list of of subreddit functionality can be found [here](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html#).\n",
        "\n",
        "\n",
        "Within each post, we can get items like the ```title```, ```url```, and ```author```. More interestingly, we can also traverse through the comments and obtain information about each comment in similar fashion.\n",
        "\n",
        "In the code below, you will notice a ```limit``` value set for both ```subreddit.hot()``` and ```post.comments.replace_more()```. That is because these functions create lists to iterate on, and creating a list with all posts/comments will likely be too much information to handle and will cause the underlying Reddit API to time out. Typically limits up to 2000 will be fine, but it may take some experimentation to set correctly. An alternative would be to set the limit to ```None``` and run the script until the API sends a ```TooManyRequests``` error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bdaJIliv4ZMf",
        "outputId": "82da6356-5708-4f32-a43f-50f5ba6f3655"
      },
      "outputs": [],
      "source": [
        "reddit = praw.Reddit(user_agent=True, client_id=\"<client_id>\", client_secret=\"<client_secret>\", username=\"<username>\", password=\"<password>\", check_for_async=False)\n",
        "\n",
        "subreddit = reddit.subreddit(\"worldnews\")\n",
        "print(subreddit.title)\n",
        "for post in subreddit.hot(limit=50):\n",
        "  print(f\"Post: {post.title}\")\n",
        "  #print(post.url)\n",
        "  print(\"---------\")\n",
        "  post.comments.replace_more(limit=100)\n",
        "  for comment in post.comments.list():\n",
        "    print(f\"\\t{comment.author}:\\'{comment.body}\\'\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCYYTd16yiid"
      },
      "source": [
        "### Redditors\n",
        "\n",
        "You can also use Praw to get information about specific Redditors. As shown above, this can be done through the ```.author``` parameter of a post, but if we are interested in a specific Redditor, then we can use the ```.redditor(<username>)``` function to give us a Redditor object to collect data from. As alluded to earlier, note how ```limit``` can be set to ```None``` if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imZvAVKs0hOK",
        "outputId": "cd4d311f-d391-43d1-9af7-e544aadb27f1"
      },
      "outputs": [],
      "source": [
        "redditor = reddit.redditor(\"helix2d\")\n",
        "for comment in redditor.comments.new(limit=None):\n",
        "    print(comment.body)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06kZ7AIQv0BT"
      },
      "source": [
        "## Putting it in a ```.csv```\n",
        "\n",
        "Now that we can retrieve information from Reddit, we can store that data for further processing using Python's ```io``` library. Due to the variety of characters possible in a comment, we will first preprocess entries to clean them of any tokens that may break the ```.csv``` format and then write them into the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nq8vOZ1g2P0U"
      },
      "outputs": [],
      "source": [
        "subreddit = reddit.subreddit(\"worldnews\")\n",
        "#print(subreddit.title)\n",
        "csv_file_path = 'data.csv'\n",
        "file = open(csv_file_path, mode='w')\n",
        "file.write(\"Username,Comment\\n\")\n",
        "for post in subreddit.hot(limit=5):\n",
        "  post.comments.replace_more(limit=10)\n",
        "  for comment in post.comments.list():\n",
        "    sbuilder = io.StringIO()\n",
        "    clean_body = comment.body.replace(\"\\n\", \"<newline>\")\n",
        "    clean_body = clean_body.replace(\",\", \"<comma>\")\n",
        "    clean_body = clean_body.replace(\"\\\"\", \"<quote>\")\n",
        "    clean_body = clean_body.replace(\"\\'\", \"<squote>\")\n",
        "    #print(f\"{comment.author},\\'{clean_body}\\'\\n\")\n",
        "    sbuilder.write(f\"{comment.author},\\'{clean_body}\\'\\n\")\n",
        "    stri = sbuilder.getvalue()\n",
        "    file.write(stri)\n",
        "    sbuilder.close()\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtO5dm6j1W-M"
      },
      "source": [
        "## Further Learning\n",
        "\n",
        "Documentation of the Praw library and its extended features can be found [here](https://praw.readthedocs.io/en/stable/index.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
